# CNN-Builder-Information-System
Информационная система для создания собственных сверточных сетей (бета)

![image](https://github.com/fenssison112/CNN-Bilder-Information-System/assets/109478845/b6c1417d-70a7-4b17-a2b3-1e514840428d)
![image](https://github.com/fenssison112/CNN-Bilder-Information-System/assets/109478845/fd33df3b-adee-4879-8d27-91a070aaba2e)
![image](https://github.com/fenssison112/CNN-Bilder-Information-System/assets/109478845/b1158307-5738-4ba7-bb06-aa8b5f2d40ed)
![image](https://github.com/fenssison112/CNN-Bilder-Information-System/assets/109478845/7e1d1d3f-5ad6-4e1a-a37f-fc619abea3c2)
![image](https://github.com/fenssison112/CNN-Bilder-Information-System/assets/109478845/d46e3e83-f1f0-4f57-9a26-3a609fc91850)
![image](https://github.com/fenssison112/CNN-Bilder-Information-System/assets/109478845/4fe78205-1aa5-44af-b2ec-1d8aadee0e6c)
![image](https://github.com/fenssison112/CNN-Bilder-Information-System/assets/109478845/a49f074c-cc35-4c2c-9842-05381541b647)

Левый модуль имеет «базовые настройки» и «продвинутые настройки». Базовые настройки подойдут людям которые имеют представление об нейронных сетях. 
В разделе размер изображения, собственно, выбирается размер изображений, на которых будет обучена модель. Число каналов отвечают за цвет изображения 1- только красный, 2- только зеленый, 3 Синий. Размер и канал формирует входной слой, который отображается в центральном окне в верху. 

Количество скрытых слоев отвечает за количество скрытых слоев в нейронной сети. 1 Скрытый слой это 1 слой свертки и 1 слой макс пулинга. Первые слои свертки имеют количество фильтров 16 и матрица 3 на 3. Слой макспулинга имеет матрицу 2 на 2. Размер фильтров с лоях свертки увеличивается через 2 слоя свертки. То есть первые 2 слоя имеют фильтры 16, следующие 2 -32 фильтра, 64, 128 и так далее. Максимальное число фильтров 512. 
Количество полносвязных слоев отвечает за количество полносвяхных слоев, последний слоя будет выходным слоем.
Класс отвечает за количество классов в будующей обучающей модели. Параметр binary означает что классов будет не больше 2ух. Categorical означает что классов будет больше 2ух классов. 
Максимальное число эпох означает конечное число эпох, если не будет пройден порог по loss. 
В контексте обучения нейронных сетей, эпоха представляет собой один полный проход через все обучающие данные. Во время одной эпохи модель использует каждый образец из обучающего набора данных один раз для обновления весов и совершения коррекции ошибок.

Предположим, у вас есть обучающий набор данных, состоящий из 1000 изображений, и вы решаете обучить модель на 10 эпохах. Значит, во время каждой эпохи модель будет проходить через все 1000 изображений, обновлять веса и корректировать ошибки. Таким образом, после завершения 10 эпох модель будет пройдена по всем 1000 изображениям 10 раз.
Значение loss для завершения обучения отвечает за установления порога для окончания обучения. 
Во время обучения модели важно отслеживать и анализировать различные параметры для оценки ее производительности. Некоторые из распространенных параметров, которые могут быть использованы при обучении модели, включают:
Потери (Loss): Показывает, насколько хорошо модель предсказывает целевые значения во время обучения. Цель заключается в минимизации потерь, чтобы модель максимально точно предсказывала целевые значения.
Точность (Accuracy): Оценивает, насколько точно модель классифицирует данные. Это обычно выражается в процентах и представляет собой долю правильных предсказаний модели.
Если во время обучения loss не дошел до установленного порога, обучение будет продолжаться до максимального количества эпох.

Алгоритм обучения 
Адаптивная оценка (Adam):
Adam  - это оптимизационный алгоритм, который сочетает в себе идеи из двух других алгоритмов - адаптивного градиентного спуска (AdaGrad) и стохастического градиентного спуска с инерцией (SGD with momentum). Adam адаптивно настраивает скорость обучения для каждого параметра, и также сохраняет экспоненциально сглаженную историю градиентов. Он показывает хорошую производительность в широком спектре задач обучения нейронных сетей.

Оптимизационный олгаритм (RMSprop):
RMSprop - это оптимизационный алгоритм, который адаптивно настраивает скорость обучения для каждого параметра модели на основе истории градиентов. Алгоритм вычисляет экспоненциальное скользящее среднее квадратов градиентов и использует его для нормализации скорости обучения. RMSprop обеспечивает эффективное обучение нейронных сетей, особенно в случае различных масштабов градиентов.

Стохастический градиентный спуск (SGD):
Стохастический градиентный спуск - это простой и широко используемый алгоритм оптимизации для обучения нейронных сетей. В отличие от более сложных алгоритмов, таких как Adam или RMSprop, SGD обновляет параметры модели на основе градиента функции потерь, рассчитанного на случайно выбранном подмножестве данных (батче). SGD позволяет выполнить обновление параметров с использованием только части данных, что делает его более эффективным с точки зрения вычислений, особенно для больших наборов данных.
Кнопка «Путь к датасет» открывает окно для указания пути к данным на которых будет обучаться модель. Важно чтобы папка имела подпапки train и val(validation). 
Кнопка «Куда сохранять модель» открывает окно для указания пути для сохранения модели. 
В центральном окне можно настроить функции активации каждого слоя и количество нейроннов полносвязных слоев.

ReLU (Rectified Linear Unit): ReLU - это функция активации, используемая в нейронных сетях для введения нелинейности. Она определяется как максимум между нулем и входным значением. ReLU активируется, если входное значение положительное, и неактивирована, если входное значение отрицательное. Ее преимущества включают простоту вычислений и предотвращение проблемы затухающих градиентов, что может способствовать эффективному обучению глубоких нейронных сетей.
Сигмоида (Sigmoid): Сигмоидная функция - это нелинейная функция активации, которая преобразует входное значение в диапазоне от 0 до 1. Она используется для прогнозирования вероятностей или выполнения бинарной классификации. Сигмоида имеет S-образную форму и обладает свойством гладкости, что облегчает вычисления градиентов и их распространение в обратном процессе обратного распространения ошибки.
Софтмакс (Softmax): Софтмакс - это функция активации, применяемая в многоклассовой классификации. Она преобразует входные значения в вероятности, распределенные по классам. Софтмакс гарантирует, что сумма вероятностей всех классов равна 1. Это позволяет интерпретировать выходные значения нейронной сети как вероятности принадлежности к каждому классу и использовать их для принятия решений о классификации.


В продвинутых настройках настройки немного меняются. Например вместо скрытого слоя появились отдельные вкладки слой свертки и шаг макспулинг. Их параметры можно настроить в центральном окне. Ядра (или фильтры) в слое свертки являются одной из ключевых составляющих сверточной нейронной сети. Они используются для извлечения определенных признаков из входных данных, таких как изображения. Каждое ядро представляет собой набор весов, которые применяются к локальной области входных данных для вычисления свертки.
Ядра выполняют две основные функции:
Извлечение признаков: Ядра применяются к различным частям входных данных, чтобы выделить различные признаки, такие как границы, текстуры, формы и другие характеристики. Каждое ядро специализируется на определенном типе признаков. Например, одно ядро может быть настроено на обнаружение вертикальных границ, а другое - на обнаружение горизонтальных границ. Путем применения различных ядер, слой свертки создает карту признаков, которая содержит информацию о выделенных характеристиках во входных данных.
Уменьшение размерности: При применении свертки с ядром к входным данным происходит операция свертки, которая приводит к уменьшению размерности данных. Это происходит за счет применения ядра к локальным областям входных данных и вычисления их суммы или среднего значения. Таким образом, каждый пиксель на выходе сверточного слоя представляет собой сумму или среднее значение пикселей, обработанных данной областью ядра. Это позволяет сети изучать более абстрактные и компактные представления данных, снижая размерность и учитывая только наиболее важные признаки.

Размер матрицы в слое свертки (также называемый размером ядра или фильтра) определяет локальную область входных данных, на которую применяется операция свертки. Размер матрицы определяет ширину и высоту этой области.
Когда сверточное ядро проходит по входным данным, оно перемещается с определенным шагом (шаг свертки) по горизонтали и вертикали и выполняет операцию свертки для каждой локальной области, на которую оно накладывается.
Размер матрицы в слое свертки является одним из гиперпараметров, которые выбираются при проектировании нейронной сети. Варьируя размер матрицы, мы можем контролировать, какие признаки будут извлечены из входных данных. 
Более маленькие размеры матрицы позволяют выделять более локальные и мелкие признаки, такие как границы или текстуры, в то время как большие размеры матрицы позволяют выделять более глобальные и абстрактные признаки, такие как формы или объекты целиком.
Каждый параметр можно настроить. 

Шаг макс пулинг отвечает через сколько слоев сверток будет добавлен слой макспулинг.
Dropout - это не слой, а метод регуляризации, который применяется в нейронных сетях. Он используется для борьбы с переобучением и повышения обобщающей способности модели.
Dropout работает путем случайного "выключения" (отключения) некоторых нейронов во время обучения. Каждый нейрон имеет вероятность (обычно выбирается в диапазоне от 0,2 до 0,5) быть выключенным на каждой итерации обучения. В результате, некоторые нейроны игнорируются и их вклад в передачу информации и вычисление градиентов уменьшается.
Применение Dropout позволяет сети обучаться более устойчиво и уменьшает взаимозависимость между нейронами, что способствует лучшей обобщающей способности модели. Это помогает предотвратить переобучение, когда модель слишком хорошо запоминает тренировочные данные и плохо обобщает на новые данные.
Обычно Dropout применяется после полносвязных слоев в нейронной сети, но его можно использовать и после сверточных слоев. 
Важно заметить, что Dropout не является обязательным элементом в нейронных сетях, и его использование зависит от конкретной задачи и архитектуры сети.


Batch Size означает, сколько образцов данных модель будет обрабатывать одновременно перед обновлением своих весов. Вместо того чтобы обрабатывать все данные сразу, мы разделяем их на небольшие пакеты и обновляем веса модели после каждого пакета
Например, если вы установите размер пакета равным 10, то модель будет обрабатывать по 10 фотографий за раз. Затем она обновит свои веса на основе результатов этой группы фотографий. Этот процесс будет повторяться для каждого пакета фотографий, пока все 1000 фотографий не будут обработаны. Это помогает ускорить обучение и эффективно использовать ресурсы вашего компьютера или сервера.

Кнопка «применить» отвечает за принятие настроек созданной архитектуры. Также происходит проверка на отрицательные слои свертки. Если ошибок нет можно нажать на кнопку «начать обучение». После начала обучения в нижнем окне появляются результаты обучения каждой эпохи. Если по каким то причинам пользователь посчитает что нужно остановить обучение, он может нажать на кнопку и обучение будет остановлена на том моменте когда была нажата кнопка. 










